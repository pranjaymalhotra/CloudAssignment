#!/bin/bash

# GCP Deployment Script for Flink Analytics
# This script deploys the Flink stream processing job to GCP Dataproc

set -e

echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo "โ       GCP Flink Deployment - E-Commerce Analytics            โ"
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo ""

# Check if gcloud is installed
if ! command -v gcloud &> /dev/null; then
    echo "โ gcloud CLI not found. Please install it first:"
    echo "   brew install --cask google-cloud-sdk"
    exit 1
fi

# Check if Maven is installed
if ! command -v mvn &> /dev/null; then
    echo "โ Maven not found. Please install it first:"
    echo "   brew install maven"
    exit 1
fi

echo "โ Prerequisites checked"
echo ""

# Get GCP Project ID
echo "Enter your GCP Project ID:"
read -p "Project ID: " PROJECT_ID

if [ -z "$PROJECT_ID" ]; then
    echo "โ Project ID cannot be empty"
    exit 1
fi

# Set project
echo "Setting GCP project to: $PROJECT_ID"
gcloud config set project $PROJECT_ID

# Variables
REGION="us-central1"
CLUSTER="analytics-cluster"
BUCKET="${PROJECT_ID}-flink-jobs"

echo ""
echo "Configuration:"
echo "  Project ID: $PROJECT_ID"
echo "  Region: $REGION"
echo "  Cluster: $CLUSTER"
echo "  Bucket: $BUCKET"
echo ""

# Step 1: Enable required APIs
echo "โถ Step 1/7: Enabling required GCP APIs..."
gcloud services enable compute.googleapis.com
gcloud services enable storage-api.googleapis.com
gcloud services enable dataproc.googleapis.com
echo "โ APIs enabled"
echo ""

# Step 2: Deploy GCP infrastructure with Terraform
echo "โถ Step 2/7: Deploying GCP infrastructure (Dataproc cluster)..."
cd terraform/gcp

# Update variables.tf with project ID
cat > terraform.tfvars <<EOF
project_id = "$PROJECT_ID"
region     = "$REGION"
EOF

terraform init
terraform apply -auto-approve

BUCKET=$(terraform output -raw flink_jobs_bucket_name)
echo "โ Infrastructure deployed"
echo "  Cluster: $CLUSTER"
echo "  Bucket: $BUCKET"
echo ""

cd ../..

# Step 3: Build Flink job
echo "โถ Step 3/7: Building Flink job JAR..."
cd analytics
mvn clean package -DskipTests

if [ ! -f target/flink-analytics-1.0.0.jar ]; then
    echo "โ Failed to build JAR"
    exit 1
fi

echo "โ JAR built: target/flink-analytics-1.0.0.jar"
echo ""

# Step 4: Upload JAR to Cloud Storage
echo "โถ Step 4/7: Uploading JAR to Cloud Storage..."
gsutil cp target/flink-analytics-1.0.0.jar gs://$BUCKET/
echo "โ JAR uploaded to gs://$BUCKET/"
echo ""

cd ..

# Step 5: Get AWS MSK Kafka brokers
echo "โถ Step 5/7: Getting AWS MSK Kafka brokers..."
cd terraform/aws

if [ ! -d ".terraform" ]; then
    echo "โ๏ธ  AWS infrastructure not found. Please deploy AWS first!"
    echo "   cd terraform/aws && terraform init && terraform apply"
    exit 1
fi

KAFKA_BROKERS=$(terraform output -raw msk_bootstrap_servers 2>/dev/null || echo "")

if [ -z "$KAFKA_BROKERS" ]; then
    echo "โ Could not get Kafka brokers. Please check AWS deployment."
    echo ""
    echo "Manual setup required:"
    echo "1. Get MSK bootstrap servers from AWS console"
    echo "2. Submit Flink job manually with:"
    echo ""
    echo "   gcloud dataproc jobs submit flink \\"
    echo "     --cluster=$CLUSTER \\"
    echo "     --region=$REGION \\"
    echo "     --jar=gs://$BUCKET/flink-analytics-1.0.0.jar \\"
    echo "     --properties=env.KAFKA_BOOTSTRAP_SERVERS=YOUR_MSK_BROKERS \\"
    echo "     -- \\"
    echo "     --kafka-brokers YOUR_MSK_BROKERS"
    exit 1
fi

echo "โ Kafka brokers: $KAFKA_BROKERS"
echo ""

cd ../..

# Step 6: Configure MSK security group (allow GCP access)
echo "โถ Step 6/7: Configuring AWS MSK security group..."
echo ""
echo "โ๏ธ  IMPORTANT: You need to allow GCP Dataproc to access AWS MSK"
echo ""
echo "Run this command to get the MSK security group ID:"
echo "  aws kafka describe-cluster --cluster-arn YOUR_CLUSTER_ARN --query 'ClusterInfo.BrokerNodeGroupInfo.SecurityGroups' --output text"
echo ""
echo "Then run:"
echo "  aws ec2 authorize-security-group-ingress \\"
echo "    --group-id sg-XXXXXXXX \\"
echo "    --protocol tcp \\"
echo "    --port 9092 \\"
echo "    --cidr 0.0.0.0/0"
echo ""
read -p "Press Enter after configuring security group..."

# Step 7: Submit Flink job to Dataproc
echo ""
echo "โถ Step 7/7: Submitting Flink job to Dataproc..."

JOB_ID=$(gcloud dataproc jobs submit flink \
    --cluster=$CLUSTER \
    --region=$REGION \
    --jar=gs://$BUCKET/flink-analytics-1.0.0.jar \
    --properties=env.KAFKA_BOOTSTRAP_SERVERS=$KAFKA_BROKERS \
    -- \
    --kafka-brokers $KAFKA_BROKERS \
    --format=json 2>/dev/null | jq -r '.reference.jobId' || echo "")

if [ -z "$JOB_ID" ]; then
    echo "โ Failed to submit job. Trying alternative method..."
    
    gcloud dataproc jobs submit flink \
        --cluster=$CLUSTER \
        --region=$REGION \
        --jar=gs://$BUCKET/flink-analytics-1.0.0.jar \
        --properties=env.KAFKA_BOOTSTRAP_SERVERS=$KAFKA_BROKERS \
        -- \
        --kafka-brokers $KAFKA_BROKERS
    
    echo ""
    echo "Job submitted! Use this command to check status:"
    echo "  gcloud dataproc jobs list --cluster=$CLUSTER --region=$REGION"
else
    echo "โ Flink job submitted successfully!"
    echo "  Job ID: $JOB_ID"
fi

echo ""
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo "โ                   ๐ Deployment Complete! ๐                  โ"
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo ""
echo "โ GCP Dataproc cluster running"
echo "โ Flink job submitted"
echo "โ Consuming from AWS MSK Kafka"
echo ""
echo "๐ Monitor job:"
echo "   gcloud dataproc jobs list --cluster=$CLUSTER --region=$REGION"
echo ""
echo "๐ View logs:"
echo "   gcloud dataproc jobs describe <JOB_ID> --region=$REGION"
echo ""
echo "๐ Stop cluster (save costs):"
echo "   gcloud dataproc clusters stop $CLUSTER --region=$REGION"
echo ""
echo "๐งน Cleanup:"
echo "   cd terraform/gcp && terraform destroy"
echo ""
