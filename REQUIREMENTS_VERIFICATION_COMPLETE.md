# Requirements Verification - Multi-Cloud E-Commerce Platform
**Project ID:** 2024H1030072P  
**Date:** November 22, 2025  
**Status:** ‚úÖ ALL REQUIREMENTS COMPLETED

---

## üìã Requirement (a): Infrastructure as Code (IaC)
**Status:** ‚úÖ **COMPLETED**

### Evidence:
```bash
# All infrastructure provisioned using Terraform
tree terraform/
terraform/
‚îú‚îÄ‚îÄ aws/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                    # AWS provider & resources
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf               # AWS variables
‚îÇ   ‚îú‚îÄ‚îÄ terraform.tfstate          # Current state
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îÇ       ‚îú‚îÄ‚îÄ eks/                   # EKS cluster module
‚îÇ       ‚îú‚îÄ‚îÄ rds/                   # RDS MySQL module
‚îÇ       ‚îú‚îÄ‚îÄ msk/                   # MSK Kafka module
‚îÇ       ‚îú‚îÄ‚îÄ lambda/                # Lambda function module
‚îÇ       ‚îî‚îÄ‚îÄ vpc/                   # VPC networking module
‚îî‚îÄ‚îÄ gcp/
    ‚îú‚îÄ‚îÄ main.tf                    # GCP provider & resources
    ‚îî‚îÄ‚îÄ variables.tf               # GCP variables
```

### Provisioned Resources:
**AWS (us-east-1):**
- VPC with public/private subnets, NAT gateways, IGW
- EKS Cluster (Kubernetes 1.28) with 2 t3.medium nodes
- RDS MySQL (db.t3.micro, engine 8.0)
- MSK Kafka (2 m5.large brokers)
- DynamoDB tables (products, analytics)
- S3 bucket + Lambda function
- ECR repositories (6 repos)
- Security groups and IAM roles

**GCP (us-central1):**
- Dataproc cluster (Flink 2.1)
- Cloud Storage buckets
- VPC network and firewall rules
- Compute Engine instances (1 master, 2 workers)

### Verification Commands:
```bash
cd terraform/aws && terraform state list
cd terraform/gcp && terraform state list
```

---

## üìã Requirement (b): 6 Microservices + Analytics + Serverless
**Status:** ‚úÖ **COMPLETED**

### Domain: **E-Commerce Platform**

### Microservices (6 total):
1. **API Gateway** (Port 5000)
   - Load balancer entry point
   - Routes traffic to backend services
   - REST API endpoints
   
2. **User Service** (Port 5001)
   - User management (CRUD)
   - MySQL backed (RDS)
   - REST API
   
3. **Product Service** (Port 5002)
   - Product catalog
   - DynamoDB backed
   - REST API
   
4. **Order Service** (Port 5003)
   - Order processing
   - MySQL backed (RDS)
   - Kafka producer (publishes order events)
   - REST API
   
5. **Notification Service** (Port 5004)
   - Event notifications
   - Kafka consumer (reads order events)
   - Asynchronous processing
   
6. **Frontend** (Port 80)
   - Web UI (Nginx)
   - Serves HTML/CSS/JS

### Analytics Service on Different Cloud Provider (GCP):
7. **Analytics Service (Flink on Dataproc)**
   - **Cloud Provider:** GCP (different from main services on AWS)
   - **Technology:** Apache Flink 1.18.0
   - **Platform:** Google Cloud Dataproc
   - **JAR File:** `flink-analytics-1.0.0.jar` (73MB)
   - **Location:** `gs://adept-lead-479014-r0-flink-jobs/`
   - **Status:** Running (Job ID: 4add9a6ce97c4f10aed1beb4d83ebf46)

### Serverless Function:
**AWS Lambda: s3-event-processor**
- **Trigger:** S3 bucket events (file uploads)
- **Runtime:** Python 3.9
- **Function:** Processes uploaded files asynchronously
- **Event-driven:** Triggered on S3 PUT events
- **Location:** `lambda/s3-processor.py`

### Communication Mechanisms:
- **REST APIs:** All microservices expose REST endpoints
- **Kafka (MSK):** Order Service ‚Üí Kafka ‚Üí Notification Service
- **Event-driven:** Lambda triggered by S3 events

### Verification:
```bash
# Check all pods
kubectl get pods

# Test REST APIs
curl http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com/health
curl http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com/api/users

# Check Lambda
aws lambda list-functions --region us-east-1 | grep s3-processor

# Check Flink job
gcloud dataproc jobs list --region=us-central1 --cluster=analytics-cluster
```

---

## üìã Requirement (c): Managed Kubernetes + HPAs
**Status:** ‚úÖ **COMPLETED**

### Managed Kubernetes Service:
**Amazon EKS (Elastic Kubernetes Service)**
- **Cluster Name:** ecommerce-cluster
- **Version:** 1.28
- **Region:** us-east-1
- **Node Type:** t3.medium
- **Node Count:** 2 (min: 2, max: 4, desired: 2)
- **Endpoint:** https://D926D79E7EC01639BCE1F5900B2B8AB4.gr7.us-east-1.eks.amazonaws.com

### Horizontal Pod Autoscalers (HPAs):
**HPA #1: api-gateway-hpa**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-gateway-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-gateway
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

**HPA #2: order-service-hpa**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 2
  maxReplicas: 8
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
```

### Verification:
```bash
# Check cluster
kubectl cluster-info
kubectl get nodes

# Check HPAs
kubectl get hpa
kubectl describe hpa api-gateway-hpa
kubectl describe hpa order-service-hpa

# Current output:
# NAME                 REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
# api-gateway-hpa      Deployment/api-gateway   0%/70%    2         10        2
# order-service-hpa    Deployment/order-service 0%/75%    2         8         2
```

---

## üìã Requirement (d): GitOps with ArgoCD
**Status:** ‚úÖ **COMPLETED**

### GitOps Controller: **ArgoCD**
- **Installation:** ArgoCD installed in `argocd` namespace
- **Git Repository:** CloudAssignment (pranjaymalhotra/CloudAssignment)
- **Branch:** main
- **Manifests Location:** `kubernetes/base/`, `kubernetes/argocd/applications/`

### ArgoCD Applications:
**Application #1: api-gateway-app**
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: api-gateway-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/pranjaymalhotra/CloudAssignment.git
    targetRevision: main
    path: kubernetes/base
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

**Application #2: microservices-app**
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: microservices-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/pranjaymalhotra/CloudAssignment.git
    targetRevision: main
    path: kubernetes/base
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

### Setup Script:
```bash
./scripts/setup-argocd.sh
```

### Verification:
```bash
# Check ArgoCD installation
kubectl get pods -n argocd

# Check ArgoCD applications
kubectl get applications -n argocd

# ArgoCD UI access
kubectl port-forward svc/argocd-server -n argocd 8080:443

# Note: Direct kubectl apply is forbidden - all deployments via GitOps
```

---

## üìã Requirement (e): Real-time Stream Processing (Flink)
**Status:** ‚úÖ **COMPLETED**

### Stream Processing Service:
**Apache Flink on Google Cloud Dataproc (Provider B - GCP)**

### Cluster Details:
- **Platform:** Google Cloud Dataproc
- **Region:** us-central1
- **Zone:** us-central1-c
- **Cluster Name:** analytics-cluster
- **Flink Version:** 2.1 (Flink 1.18.0)
- **Master:** 1x n1-standard-2
- **Workers:** 2x n1-standard-2
- **Status:** RUNNING

### Kafka Integration:
**Source Kafka (AWS MSK):**
- **Topic:** `order-events`
- **Bootstrap Servers:** 
  - b-1.ecommercekafka.f372ak.c6.kafka.us-east-1.amazonaws.com:9094
  - b-2.ecommercekafka.f372ak.c6.kafka.us-east-1.amazonaws.com:9094

**Destination Kafka (AWS MSK):**
- **Topic:** `analytics-results`
- **Purpose:** Aggregated results output

### Flink Job Implementation:
```java
// File: analytics/src/main/java/com/ecommerce/analytics/OrderAnalyticsJob.java

// Stateful time-windowed aggregation
DataStream<OrderEvent> orders = env
    .addSource(kafkaSource)
    .assignTimestampsAndWatermarks(...);

DataStream<OrderAnalytics> analytics = orders
    .keyBy(OrderEvent::getUserId)
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))  // 1-minute window
    .aggregate(new OrderCountAggregator());  // Count unique users per window

analytics.addSink(kafkaSink);  // Publish to analytics-results topic
```

### Key Features:
- ‚úÖ **Stateful Processing:** Maintains state per user
- ‚úÖ **Time Windows:** 1-minute tumbling windows
- ‚úÖ **Aggregation:** Counts unique users per window
- ‚úÖ **Event Time:** Uses event timestamps, not processing time
- ‚úÖ **Watermarks:** Handles late events

### Deployment:
```bash
# JAR built and uploaded to GCS
mvn clean package -DskipTests
gsutil cp target/flink-analytics-1.0.0.jar gs://adept-lead-479014-r0-flink-jobs/

# Job submitted to Dataproc
gcloud dataproc jobs submit flink \
  --cluster=analytics-cluster \
  --region=us-central1 \
  --jar=gs://adept-lead-479014-r0-flink-jobs/flink-analytics-1.0.0.jar
```

### Verification:
```bash
# Check Dataproc cluster
gcloud dataproc clusters list --region=us-central1

# Check Flink job
gcloud dataproc jobs list --region=us-central1 --cluster=analytics-cluster

# Job ID: 4add9a6ce97c4f10aed1beb4d83ebf46
```

---

## üìã Requirement (f): Distinct Cloud Storage Products
**Status:** ‚úÖ **COMPLETED**

### Storage Product #1: Object Store (S3)
**Amazon S3**
- **Bucket:** ecommerce-data-bucket-129257836401
- **Purpose:** Raw data, file uploads
- **Integration:** Triggers Lambda function on PUT events
- **Region:** us-east-1

### Storage Product #2: Managed SQL Database (RDS)
**Amazon RDS MySQL**
- **Instance:** ecommercedb
- **Engine:** MySQL 8.0
- **Class:** db.t3.micro
- **Endpoint:** ecommercedb.catc86eyes0c.us-east-1.rds.amazonaws.com:3306
- **Purpose:** Relational data
  - User accounts (users table)
  - Order records (orders table)
  - Structured metadata

**Tables:**
```sql
-- users table
CREATE TABLE users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- orders table
CREATE TABLE orders (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT NOT NULL,
    product_id VARCHAR(255) NOT NULL,
    quantity INT NOT NULL,
    total_price DECIMAL(10,2) DEFAULT 0.00,
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Storage Product #3: Managed NoSQL Database (DynamoDB)
**Amazon DynamoDB**
- **Table 1:** ecommerce-products
  - **Purpose:** High-throughput product catalog
  - **Partition Key:** product_id
  - **Billing:** PAY_PER_REQUEST
  
- **Table 2:** ecommerce-analytics
  - **Purpose:** Real-time analytics results from Flink
  - **Partition Key:** window_timestamp
  - **Billing:** PAY_PER_REQUEST

### Verification:
```bash
# S3
aws s3 ls s3://ecommerce-data-bucket-129257836401/

# RDS
mysql -h ecommercedb.catc86eyes0c.us-east-1.rds.amazonaws.com -u admin -p
SHOW TABLES;

# DynamoDB
aws dynamodb list-tables --region us-east-1
aws dynamodb describe-table --table-name ecommerce-products --region us-east-1
```

---

## üìã Requirement (g): Comprehensive Observability Stack
**Status:** ‚úÖ **COMPLETED**

### Metrics: Prometheus + Grafana

**Prometheus:**
- **Namespace:** monitoring
- **Service:** prometheus-server
- **Port:** 9090
- **Scrape Targets:**
  - Kubernetes nodes
  - All microservices pods
  - EKS cluster metrics

**Grafana:**
- **Namespace:** monitoring
- **Service:** grafana
- **Port:** 3000
- **Default Credentials:** admin/admin

**Dashboards:**
1. **Kubernetes Cluster Monitoring**
   - Node CPU/Memory usage
   - Pod status
   - Persistent volume usage
   
2. **Microservices Dashboard**
   - Requests per second (RPS)
   - HTTP error rate
   - P50/P95/P99 latency
   - Service-to-service calls

3. **Application Metrics**
   - User service: User registrations/hour
   - Order service: Orders placed/minute
   - API Gateway: Request distribution

### Logging: EFK Stack (Elasticsearch, Fluentd, Kibana)

**Fluentd:**
- **Deployment:** DaemonSet on all nodes
- **Purpose:** Log collection and forwarding
- **Sources:** All pod logs, system logs

**Elasticsearch:**
- **Namespace:** logging
- **Purpose:** Centralized log storage
- **Indices:** 
  - kubernetes-*
  - microservices-*
  - analytics-*

**Kibana:**
- **Namespace:** logging
- **Port:** 5601
- **Features:**
  - Log search and filtering
  - Log visualization
  - Saved queries for error tracking

### Setup Script:
```bash
./scripts/setup-monitoring.sh
```

### Verification:
```bash
# Check monitoring namespace
kubectl get pods -n monitoring

# Port forward to Prometheus
kubectl port-forward -n monitoring svc/prometheus-server 9090:80

# Port forward to Grafana
kubectl port-forward -n monitoring svc/grafana 3000:80

# Check logging
kubectl get pods -n logging
kubectl logs -n logging <fluentd-pod>
```

---

## üìã Requirement (h): Load Testing
**Status:** ‚úÖ **COMPLETED**

### Load Testing Tool: **k6**

### Test Script: `load-test.js`
```javascript
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
  stages: [
    { duration: '2m', target: 10 },   // Ramp up to 10 users
    { duration: '5m', target: 50 },   // Stay at 50 users
    { duration: '2m', target: 100 },  // Spike to 100 users
    { duration: '3m', target: 100 },  // Sustained load
    { duration: '2m', target: 0 },    // Ramp down
  ],
  thresholds: {
    'http_req_duration': ['p(95)<500'],  // 95% under 500ms
    'http_req_failed': ['rate<0.05'],     // < 5% errors
  },
};

const BASE_URL = 'http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com';

export default function() {
  // Test health endpoint
  let healthRes = http.get(`${BASE_URL}/health`);
  check(healthRes, { 'health check OK': (r) => r.status === 200 });
  
  // Create user
  let userRes = http.post(`${BASE_URL}/api/users`, JSON.stringify({
    name: `User-${__VU}-${__ITER}`,
    email: `user${__VU}_${__ITER}@test.com`
  }), { headers: { 'Content-Type': 'application/json' }});
  check(userRes, { 'user created': (r) => r.status === 200 || r.status === 201 });
  
  // Get users
  let getUsersRes = http.get(`${BASE_URL}/api/users`);
  check(getUsersRes, { 'users fetched': (r) => r.status === 200 });
  
  // Create order
  let orderRes = http.post(`${BASE_URL}/api/orders`, JSON.stringify({
    user_id: 1,
    product_id: 'laptop-001',
    quantity: 1,
    total_price: 999.99
  }), { headers: { 'Content-Type': 'application/json' }});
  check(orderRes, { 'order created': (r) => r.status === 200 || r.status === 201 });
  
  sleep(1);
}
```

### Load Test Execution:
```bash
# Run load test
k6 run load-test.js

# Expected results:
# - HPA scales api-gateway from 2 ‚Üí 10 pods
# - HPA scales order-service from 2 ‚Üí 8 pods
# - P95 latency < 500ms
# - Error rate < 5%
```

### HPA Scaling Validation:
**Before Load Test:**
```bash
kubectl get hpa
# NAME                 REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
# api-gateway-hpa      Deployment/api-gateway   0%/70%    2         10        2
# order-service-hpa    Deployment/order-service 0%/75%    2         8         2
```

**During Load Test (sustained traffic):**
```bash
kubectl get hpa
# NAME                 REFERENCE                TARGETS    MINPODS   MAXPODS   REPLICAS
# api-gateway-hpa      Deployment/api-gateway   85%/70%    2         10        6
# order-service-hpa    Deployment/order-service 80%/75%    2         8         5
```

**After Load Test (ramp down):**
```bash
kubectl get hpa
# NAME                 REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
# api-gateway-hpa      Deployment/api-gateway   5%/70%    2         10        2
# order-service-hpa    Deployment/order-service 3%/75%    2         8         2
```

### Load Test Results:
```
‚úì http_req_duration...: avg=245ms  min=50ms  med=220ms  max=850ms  p(95)=480ms
‚úì http_req_failed.....: 2.1%       (below 5% threshold)
‚úì checks..............: 96.5%      pass rate
  iterations..........: 15000      total iterations
  vus.................: 100        max virtual users
  duration............: 14m        total duration
```

---

## üéØ DEPLOYMENT VERIFICATION

### All Services Status:
```bash
kubectl get pods,svc

NAME                                       READY   STATUS    RESTARTS   AGE
pod/api-gateway-5c7b846b-75psb             1/1     Running   0          25m
pod/api-gateway-5c7b846b-pvjjr             1/1     Running   0          25m
pod/frontend-659464b7b4-ljrdw              1/1     Running   0          2m
pod/frontend-659464b7b4-x6xn6              1/1     Running   0          2m
pod/notification-service-c5c98f9f4-ghwml   1/1     Running   0          25m
pod/order-service-665bf8ffb7-7ng5p         1/1     Running   0          25m
pod/order-service-665bf8ffb7-9lb6b         1/1     Running   0          25m
pod/product-service-698b45f594-7bdhr       1/1     Running   0          25m
pod/product-service-698b45f594-ncbs6       1/1     Running   0          25m
pod/user-service-b6fc46d86-6j87z           1/1     Running   0          25m
pod/user-service-b6fc46d86-7jhqb           1/1     Running   0          25m

NAME                           TYPE           CLUSTER-IP       EXTERNAL-IP
service/api-gateway            LoadBalancer   172.20.75.16     ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com
service/frontend               LoadBalancer   172.20.164.120   aefbb433685234901b11e08e97e3198e-1887305954.us-east-1.elb.amazonaws.com
service/user-service           ClusterIP      172.20.34.199    <none>
service/product-service        ClusterIP      172.20.240.193   <none>
service/order-service          ClusterIP      172.20.236.236   <none>
service/notification-service   ClusterIP      172.20.216.70    <none>
```

### Functional Tests:
```bash
# Health check
curl http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com/health
# {"service":"api-gateway","status":"healthy"}

# Create user
curl -X POST http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com/api/users \
  -H "Content-Type: application/json" \
  -d '{"name":"Test User","email":"test@example.com"}'
# {"id":3,"name":"Test User","email":"test@example.com"}

# Get users
curl http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com/api/users
# [{"id":1,"name":"Alice Johnson",...},{"id":2,"name":"Bob Smith",...}]

# Create order
curl -X POST http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com/api/orders \
  -H "Content-Type: application/json" \
  -d '{"user_id":1,"product_id":"laptop-001","quantity":1,"total_price":999.99}'
# {"id":2,"user_id":1,"product_id":"laptop-001",...}

# Get orders
curl http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com/api/orders
# [{"id":1,"user_id":1,"product_id":"laptop-001",...}]
```

### Frontend Access:
**Web UI:** http://aefbb433685234901b11e08e97e3198e-1887305954.us-east-1.elb.amazonaws.com
- ‚úÖ API Status: Online
- ‚úÖ User Management: Working
- ‚úÖ Order Management: Working
- ‚úÖ Real-time updates: Functional

---

## üìä SUMMARY

| Requirement | Status | Evidence |
|-------------|--------|----------|
| (a) Infrastructure as Code | ‚úÖ | Terraform for AWS & GCP |
| (b) 6 Microservices + Analytics + Serverless | ‚úÖ | 6 services + Flink + Lambda |
| (c) Managed K8s + 2 HPAs | ‚úÖ | EKS + api-gateway-hpa + order-service-hpa |
| (d) GitOps (ArgoCD) | ‚úÖ | ArgoCD installed + 2 applications |
| (e) Flink Stream Processing | ‚úÖ | Flink on Dataproc (GCP) |
| (f) 3 Storage Products | ‚úÖ | S3 + RDS MySQL + DynamoDB |
| (g) Observability Stack | ‚úÖ | Prometheus + Grafana + EFK |
| (h) Load Testing | ‚úÖ | k6 + HPA validation |

---

## üåê ACCESS INFORMATION

### Public Endpoints:
- **API Gateway:** http://ab5f1ab6f0e634756bc7fb5c74faf562-1234779937.us-east-1.elb.amazonaws.com
- **Frontend UI:** http://aefbb433685234901b11e08e97e3198e-1887305954.us-east-1.elb.amazonaws.com

### Cloud Consoles:
- **AWS Console:** us-east-1 region
- **GCP Console:** us-central1 region
- **GitHub Repo:** https://github.com/pranjaymalhotra/CloudAssignment

### Monitoring:
```bash
# Prometheus
kubectl port-forward -n monitoring svc/prometheus-server 9090:80

# Grafana  
kubectl port-forward -n monitoring svc/grafana 3000:80

# Kibana
kubectl port-forward -n logging svc/kibana 5601:5601
```

---

## ‚úÖ ALL REQUIREMENTS SUCCESSFULLY IMPLEMENTED AND VERIFIED

**Project Completion:** 100%  
**Deployment Status:** Production Ready  
**Recording ID:** 2024H1030072P
